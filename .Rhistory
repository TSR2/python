content = read.tbale("C:\Users\TSR\Desktop\語料庫\正面情感词语（中文）.txt")
content = read.tbale("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt")
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt")
head(content)
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
Encoding = "UTF-8")
?read.table
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
Encoding = "UTF8")
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
encoding = "UTF8")
head(content)
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
encoding = "UTF-8")
head(content)
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
encoding = "UTF-8")
head(content)
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
encoding = "UTF-8")
head(content)
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
encoding = "utf-8")
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",encoding = "utf-8")
head(content)
content = read.table("C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
encoding = "utf-8", header = F)
content = read.table("C:/Users/TSR/Desktop/語料庫/负面情感词语（中文）.txt",
encoding = "utf-8", header = F)
content = read.table("C:/Users/TSR/Desktop/語料庫/tt1.txt",
encoding = "utf-8", header = F)
content = read.table("C:/Users/TSR/Desktop/語料庫/tt1.txt")
library(dplyr)
content = read.table("C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",
encoding = "utf-8", header = F)
content = read.line("C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",
encoding = "utf-8", header = F)
head(content)
?readLines
?readLine
?readline
content = readlines("C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",
encoding = "utf-8")
content = readLines("C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",
encoding = "utf-8")
head(content)
content = readLines("C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",
encoding = "GB2312")
head(content)
content = readLines("C:\\Users\\TSR\\Desktop\\語料庫\\正面情感词语（中文）.txt",
encoding = "utf-8")
head(content)
content = readLines("C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",
encoding = "GBK")
head(content)
content = read.table("C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",
encoding = "GBK")
content = read.table(file = "C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",
encoding = "GBK")
content = read.table(file = "C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",encoding = "GBK")
?read.table
content = read.table(file = "C:\\Users\\TSR\\Desktop\\語料庫\\tt1.txt",
encoding = "GBK",header = F)
content = read.table(file = "C:\\Users\\TSR\\Desktop\\語料庫\\正面情感词语（中文）.txt",
encoding = "UTF-8",header = F)
content = read.table(file = "C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
encoding = "UTF-8",header = F)
content = read.table(file = "C:/Users/TSR/Desktop/語料庫/正面情感词语（中文）.txt",
encoding = "UTF-8",header = F)
head(content)
content = read.table(file = "C:/Users/TSR/Desktop/語料庫/负面情感词语（中文）.txt",
encoding = "GBK",header = F)
content = read.table(file = "C:/Users/TSR/Desktop/語料庫/负面情感词语（中文）.txt",
encoding = "GBK",header = F,skip = 3)
head(content)
library(readxl)
content = read_excel("C:/Users/TSR/Desktop/語料庫/负面情感词语（中文）.txt")
content = read_excel("C:/Users/TSR/Desktop/語料庫/ptt_tag.xlsx")
head(content)
warning()
content = read_excel("C:/Users/TSR/Desktop/語料庫/ptt_tag.xlsx")
warnings()
content = read_excel("C:/Users/TSR/Desktop/語料庫/ptt_tag.xlsx")
content = read_excel("C:/Users/TSR/Desktop/語料庫/ptt_tag.xlsx")
library(jiebaR)
library(dplyr)
library(magrittr)
wk = worker()
data_org <- read.csv("C:/Users/TSR/Desktop/project data/ptt_tag_V1.csv",
header = T, stringsAsFactors = FALSE, encoding = "big5")
len = 200
data_org$tag[1:len] %>% table
data_1 = data_org[1:len,]
data_1 = data_1[data_1$tag != 0,]
len1 = dim(data_1)[1]
set.seed(7)
lead = sample(1:len1,len1)
data = data_1
index_data = data[lead[1:floor(len1*0.9)],]
index_data_test = data[lead[(floor(len1*0.9)+1):len1],]
combine_text = index_data %>% group_by(tag) %>% summarise(text1=paste(text, collapse=""))
print(combine_text[6,2],width = 200)
split_list= apply(combine_text, 1, function(x) list(x[1] %>% as.numeric,wk[x[2]]))
split_list %>% sapply(function(x) c(x[1],length(x[[2]])))
test = index_data_test
index_data_test[1,]
index_data_test$tag
map_sentment_alan = function(y){
score = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum %>% prod(x[[1]]))  %>% sum
count = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum )  %>% sum
if (count == 0) count = 1
#print(paste("count:",count))
#print(paste("score:",score))
#print(paste("score/count",score/count))
score/count
}
ss = c()
test_size = dim(test)[1]
for (i in 1:test_size) {
test_file = wk[test[i,4]]
tt1 = test_file %>% sapply(map_sentment_alan) %>% mean
ss = c(ss,tt1)
}
index_data_test$tag
t = data.frame(index_data_test$tag[1:test_size],ss)
cor.test(index_data_test$tag[1:test_size],ss,method="pearson")
keys = worker("keywords",topn=5)
split_list %>% sapply( function(x) vector_keywords(x[[2]],keys))
dim(test)
"他是誰" %>% wk
"他是誰" %>% wk[]
sentment_predict = function(article){
#predict_list = c()
test_file = wk[article[4]]
predict_score = test_file %>% sapply(map_sentment_alan) %>% mean
#predict_list = c(predict_list,predict_score)
}
test %>% apply(1,sentment_predict)
predict_list = test %>% apply(1,sentment_predict)
predict_list
index_data_test$tag
cor.test(index_data_test$tag,predict_list,method="pearson")
index_data_test$tag
map_sentment_alan = function(y){
score = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum %>% prod(x[[1]]))  %>% sum
count = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum )  %>% sum
if (count == 0) count = 1
#print(paste("count:",count))
#print(paste("score:",score))
#print(paste("score/count",score/count))
score/count
}
sentment_predict = function(article){
test_file = wk[article[4]]
predict_score = test_file %>% sapply(map_sentment_alan) %>% mean
}
predict_list = index_data_test %>% apply(1,sentment_predict)
data.frame(index_data_test$tag,predict_list)
cor.test(index_data_test$tag,predict_list,method="pearson")
wordcloud(split_list[[1]][[2]])
library(wordcloud)
wordcloud(split_list[[1]][[2]])
split_list[[1]][[2]]
wordcloud(split_list[[5]][[2]])
?wordcloud
split_list[[5]][[2]] %>% Encoding()
split_list[[1]][[2]] %>% Encoding()
split_list[[1]][[2]]
p = split_list[[1]][[2]]
Encoding(p) = "UTF-8"
wordcloud(p)
wordcloud(p)
fc-list :lang=zh-cn
jpeg(filename='wordcloud.jpg', width=800,height=800,units='px')
wordcloud(p)
dev.off()
?par
par(family='STKaiti')
wordcloud(p)
par(family='新細名體')
wordcloud(p)
install.packages("wordcloud2")
library(wordcloud2)
wordcloud2(p)
table(p)
library(tm)
library(tmcn)
library(rJava)
library(plyr)
library(stringr)
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(Rwordseg)
library(XML)
library(RCurl)
library(jiebaR)
library(dplyr)
library(magrittr)
wk = worker()
data_org <- read.csv("C:/Users/TSR/Desktop/project data/ptt_tag_V1.csv",
header = T, stringsAsFactors = FALSE, encoding = "big5")
len = 200
data_org$tag[1:len] %>% table
data_1 = data_org[1:len,]
data_1 = data_1[data_1$tag != 0,]
len1 = dim(data_1)[1]
set.seed(7)
lead = sample(1:len1,len1)
data = data_1
index_data = data[lead[1:floor(len1*0.9)],]
index_data_test = data[lead[(floor(len1*0.9)+1):len1],]
combine_text = index_data %>% group_by(tag) %>% summarise(text1=paste(text, collapse=""))
print(combine_text[6,2],width = 200)
split_list= apply(combine_text, 1, function(x) list(x[1] %>% as.numeric,wk[x[2]]))
split_list %>% sapply(function(x) c(x[1],length(x[[2]])))
keys = worker("keywords",topn=5)
split_list %>% sapply( function(x) vector_keywords(x[[2]],keys))
#情感分析 字典(若沒有下載字典，這段不用跑)
pos = read.table("C:/Users/TSR/Desktop/project data/pos.txt",stringsAsFactors = FALSE)
pos = pos[,1]
pos[1:100]
neg = read.table("C:/Users/TSR/Desktop/project data/neg.txt",stringsAsFactors = FALSE)
neg = neg[,1]
neg[1:100]
c(1,3,4) %in% c(3,4)
map_sentment = function(x){
pos_point = x[[2]] %in% pos %>% sum
neg_point = x[[2]] %in% neg %>% sum
c(x[1] %>% as.numeric(),pos_point,neg_point)
}
test1 = split_list %>% sapply(map_sentment)
test1
test1 %<>% as.data.frame()
rbind(test1,test1[2,]/test1[3,])
#情感分析 自創
map_sentment_alan = function(y){
score = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum %>% prod(x[[1]]))  %>% sum
count = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum )  %>% sum
if (count == 0) count = 1
score/count
}
sentment_predict = function(article){
test_file = wk[article[4]]
predict_score = test_file %>% sapply(map_sentment_alan) %>% mean
}
predict_list = index_data_test %>% apply(1,sentment_predict)
data.frame(index_data_test$tag,predict_list)
cor.test(index_data_test$tag,predict_list,method="pearson")
split_list
data_org$tag[1:len] %>% table
data_1 = data_org[1:len,]
data_1 = data_1[data_1$tag != 0,]
len1 = dim(data_1)[1]
set.seed(20)
lead = sample(1:len1,len1)
data = data_1
index_data = data[lead[1:floor(len1*0.9)],]
index_data_test = data[lead[(floor(len1*0.9)+1):len1],]
combine_text = index_data %>% group_by(tag) %>% summarise(text1=paste(text, collapse=""))
print(combine_text[6,2],width = 200)
split_list= apply(combine_text, 1, function(x) list(x[1] %>% as.numeric,wk[x[2]]))
split_list %>% sapply(function(x) c(x[1],length(x[[2]])))
keys = worker("keywords",topn=5)
split_list %>% sapply( function(x) vector_keywords(x[[2]],keys))
#情感分析 字典(若沒有下載字典，這段不用跑)
pos = read.table("C:/Users/TSR/Desktop/project data/pos.txt",stringsAsFactors = FALSE)
pos = pos[,1]
pos[1:100]
neg = read.table("C:/Users/TSR/Desktop/project data/neg.txt",stringsAsFactors = FALSE)
neg = neg[,1]
neg[1:100]
c(1,3,4) %in% c(3,4)
map_sentment = function(x){
pos_point = x[[2]] %in% pos %>% sum
neg_point = x[[2]] %in% neg %>% sum
c(x[1] %>% as.numeric(),pos_point,neg_point)
}
test1 = split_list %>% sapply(map_sentment)
test1
test1 %<>% as.data.frame()
rbind(test1,test1[2,]/test1[3,])
#情感分析 自創
map_sentment_alan = function(y){
score = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum %>% prod(x[[1]]))  %>% sum
count = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum )  %>% sum
if (count == 0) count = 1
score/count
}
sentment_predict = function(article){
test_file = wk[article[4]]
predict_score = test_file %>% sapply(map_sentment_alan) %>% mean
}
predict_list = index_data_test %>% apply(1,sentment_predict)
data.frame(index_data_test$tag,predict_list)
cor.test(index_data_test$tag,predict_list,method="pearson")
data_org$tag[1:len] %>% table
data_1 = data_org[1:len,]
data_1 = data_1[data_1$tag != 0,]
len1 = dim(data_1)[1]
set.seed(30)
lead = sample(1:len1,len1)
data = data_1
index_data = data[lead[1:floor(len1*0.9)],]
index_data_test = data[lead[(floor(len1*0.9)+1):len1],]
combine_text = index_data %>% group_by(tag) %>% summarise(text1=paste(text, collapse=""))
print(combine_text[6,2],width = 200)
split_list= apply(combine_text, 1, function(x) list(x[1] %>% as.numeric,wk[x[2]]))
split_list %>% sapply(function(x) c(x[1],length(x[[2]])))
keys = worker("keywords",topn=5)
split_list %>% sapply( function(x) vector_keywords(x[[2]],keys))
#情感分析 字典(若沒有下載字典，這段不用跑)
pos = read.table("C:/Users/TSR/Desktop/project data/pos.txt",stringsAsFactors = FALSE)
pos = pos[,1]
pos[1:100]
neg = read.table("C:/Users/TSR/Desktop/project data/neg.txt",stringsAsFactors = FALSE)
neg = neg[,1]
neg[1:100]
c(1,3,4) %in% c(3,4)
map_sentment = function(x){
pos_point = x[[2]] %in% pos %>% sum
neg_point = x[[2]] %in% neg %>% sum
c(x[1] %>% as.numeric(),pos_point,neg_point)
}
test1 = split_list %>% sapply(map_sentment)
test1
test1 %<>% as.data.frame()
rbind(test1,test1[2,]/test1[3,])
#情感分析 自創
map_sentment_alan = function(y){
score = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum %>% prod(x[[1]]))  %>% sum
count = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum )  %>% sum
if (count == 0) count = 1
score/count
}
sentment_predict = function(article){
test_file = wk[article[4]]
predict_score = test_file %>% sapply(map_sentment_alan) %>% mean
}
predict_list = index_data_test %>% apply(1,sentment_predict)
data.frame(index_data_test$tag,predict_list)
cor.test(index_data_test$tag,predict_list,method="pearson")
data_org$tag[1:len] %>% table
data_1 = data_org[1:len,]
data_1 = data_1[data_1$tag != 0,]
len1 = dim(data_1)[1]
set.seed(40)
lead = sample(1:len1,len1)
data = data_1
index_data = data[lead[1:floor(len1*0.9)],]
index_data_test = data[lead[(floor(len1*0.9)+1):len1],]
combine_text = index_data %>% group_by(tag) %>% summarise(text1=paste(text, collapse=""))
print(combine_text[6,2],width = 200)
split_list= apply(combine_text, 1, function(x) list(x[1] %>% as.numeric,wk[x[2]]))
split_list %>% sapply(function(x) c(x[1],length(x[[2]])))
keys = worker("keywords",topn=5)
split_list %>% sapply( function(x) vector_keywords(x[[2]],keys))
#情感分析 字典(若沒有下載字典，這段不用跑)
pos = read.table("C:/Users/TSR/Desktop/project data/pos.txt",stringsAsFactors = FALSE)
pos = pos[,1]
pos[1:100]
neg = read.table("C:/Users/TSR/Desktop/project data/neg.txt",stringsAsFactors = FALSE)
neg = neg[,1]
neg[1:100]
c(1,3,4) %in% c(3,4)
map_sentment = function(x){
pos_point = x[[2]] %in% pos %>% sum
neg_point = x[[2]] %in% neg %>% sum
c(x[1] %>% as.numeric(),pos_point,neg_point)
}
test1 = split_list %>% sapply(map_sentment)
test1
test1 %<>% as.data.frame()
rbind(test1,test1[2,]/test1[3,])
#情感分析 自創
map_sentment_alan = function(y){
score = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum %>% prod(x[[1]]))  %>% sum
count = split_list %>% sapply(function(x) x[[2]] %in% y %>% sum )  %>% sum
if (count == 0) count = 1
score/count
}
sentment_predict = function(article){
test_file = wk[article[4]]
predict_score = test_file %>% sapply(map_sentment_alan) %>% mean
}
predict_list = index_data_test %>% apply(1,sentment_predict)
data.frame(index_data_test$tag,predict_list)
cor.test(index_data_test$tag,predict_list,method="pearson")
